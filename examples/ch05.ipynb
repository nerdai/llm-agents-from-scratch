{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60405002-e4c4-4ee2-8aae-53e704cdba1c",
   "metadata": {},
   "source": [
    "# Examples from Chapter 5 ‚Äî MCP Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb4d8c3-8137-4fd3-bf37-c5359018538e",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "\n",
    "To ensure you have the required dependencies to run this notebook, you'll need to have our `llm-agents-from-scratch` framework installed on the running Jupyter kernel. To do this, you can launch this notebook with the following command while within the project's root directory:\n",
    "\n",
    "```sh\n",
    "uv run --with jupyter jupyter lab\n",
    "```\n",
    "\n",
    "Alternatively, if you just want to use the published version of `llm-agents-from-scratch` without local development, you can install it from PyPi by uncommenting the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd15ba8-e106-486e-afb5-8deef2dee2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to install `llm-agents-from-scratch` from PyPi\n",
    "# !pip install llm-agents-from-scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf43a249-cbd0-4e70-b8d0-3fd14b4b32ac",
   "metadata": {},
   "source": [
    "## Running an Ollama service\n",
    "\n",
    "To execute the code provided in this notebook, you‚Äôll need to have Ollama installed on your local machine and have its LLM hosting service running. To download Ollama, follow the instructions found on this page: https://ollama.com/download. After downloading and installing Ollama, you can start a service by opening a terminal and running the command `ollama serve`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a94c6c-a08f-4850-8c15-dd68fb6ee4c4",
   "metadata": {},
   "source": [
    "## The Hailstone MCP server\n",
    "The examples in this notebook demonstrate how to use the framework's MCP integration using a toy MCP server that exposes the familiar Hailstone tool.\n",
    "\n",
    "**Important:** Run this notebook from within the project's root directory.\n",
    "\n",
    "The code for the MCP server is located at: https://github.com/nerdai/llm-agents-from-scratch/tree/main/extra/mcp-hailstone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f8ec9a-28e2-4e5e-942f-091e789ed14b",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6281fc2-5412-446c-8c80-3226eaeabe5a",
   "metadata": {},
   "source": [
    "### Example 1: Creating an `MCPToolProvider`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b1850b6-7b43-4f80-9b10-216ffb7a93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_agents_from_scratch.tools.mcp import MCPToolProvider\n",
    "from mcp import StdioServerParameters\n",
    "from pathlib import Path\n",
    "\n",
    "server_path = Path.cwd().parent / \"extra/mcp-hailstone\"\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"uv\",\n",
    "    args=[\"run\", \"--with\", \"mcp\", \"mcp\", \"run\", \"main.py\"],\n",
    "    cwd=server_path,\n",
    ")\n",
    "provider = MCPToolProvider(\n",
    "    name=\"hailstone\",\n",
    "    stdio_params=server_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a895652-6d4a-4474-ba07-4e898d454cd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llm_agents_from_scratch.tools.mcp.provider.MCPToolProvider at 0x7b13dabbc590>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "provider"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0f8460-0263-4cf4-9a1c-88f51f1ac0f8",
   "metadata": {},
   "source": [
    "### Example 2: Establishing a session (connection) to Hailstone MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c9f951a-d4b8-4912-b533-801eda7fbac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First call (establish): 0.289514s\n",
      "Second call (cached): 0.000057s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "provider = MCPToolProvider(\n",
    "    name=\"hailstone\",\n",
    "    stdio_params=server_params,\n",
    ")\n",
    "\n",
    "start = time.perf_counter()\n",
    "await provider.session()\n",
    "print(f\"First call (establish): {time.perf_counter() - start:.6f}s\")\n",
    "\n",
    "start = time.perf_counter()\n",
    "await provider.session()\n",
    "print(f\"Second call (cached): {time.perf_counter() - start:.6f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d9b5f4-efb1-43b0-9197-51d914334546",
   "metadata": {},
   "source": [
    "### Example 3: Discovering tools and creating `MCPTool` representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e8be908-7430-4762-b092-36c07d241083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tools discovered: 1\n"
     ]
    }
   ],
   "source": [
    "provider = MCPToolProvider(\n",
    "    name=\"hailstone\",\n",
    "    stdio_params=server_params,\n",
    ")\n",
    "\n",
    "tools = await provider.get_tools()\n",
    "print(f\"Number of tools discovered: {len(tools)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3436e43b-2a5a-4e52-94d3-dc2f8955908b",
   "metadata": {},
   "source": [
    "### Example 4: Tearing down the session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0094d236-aff8-4aec-b460-cf5669e47ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session is None: False\n",
      "Session is ready: True\n",
      "-------------\n",
      "Session is None: True\n",
      "Session is ready: False\n"
     ]
    }
   ],
   "source": [
    "provider = MCPToolProvider(\n",
    "    name=\"hailstone\",\n",
    "    stdio_params=server_params,\n",
    ")\n",
    "\n",
    "await provider.session()\n",
    "print(f\"Session is None: {provider._session is None}\")\n",
    "print(f\"Session is ready: {provider._session_ready.is_set()}\")\n",
    "\n",
    "print(\"-------------\")\n",
    "\n",
    "await provider.close()\n",
    "print(f\"Session is None: {provider._session is None}\")\n",
    "print(f\"Session is ready: {provider._session_ready.is_set()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fc73fd-f64b-41da-ab31-da05392a2196",
   "metadata": {},
   "source": [
    "### Example 5: Printing the MCP Hailstone tool's attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c42765-4e7b-4b9f-818d-f0c43151859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name: mcp__hailstone__hailstone_step_fn\n",
      "Tool description: Performs a single step of the Hailstone sequence.\n",
      "Tool parameters: {'properties': {'x': {'title': 'X', 'type': 'integer'}}, 'required': ['x'], 'title': 'hailstone_step_fnArguments', 'type': 'object'}\n",
      "Provider reference: <llm_agents_from_scratch.tools.mcp.provider.MCPToolProvider object at 0x7b13dacde190>\n",
      "Additional annotations: None\n"
     ]
    }
   ],
   "source": [
    "provider = MCPToolProvider(\n",
    "    name=\"hailstone\",\n",
    "    stdio_params=server_params,\n",
    ")\n",
    "\n",
    "tools = await provider.get_tools()\n",
    "\n",
    "print(f\"Tool name: {tools[0].name}\")\n",
    "print(f\"Tool description: {tools[0].description}\")\n",
    "print(f\"Tool parameters: {tools[0].parameters_json_schema}\")\n",
    "print(f\"Provider reference: {tools[0].provider}\")\n",
    "print(f\"Additional annotations: {tools[0].additional_annotations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3a76fd-71e7-4076-8e4d-7bd5428134ff",
   "metadata": {},
   "source": [
    "### Example 6: MCP Hailstone tool execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c898fae4-41a9-4ccf-9c36-d15b42543d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolCallResult(tool_call_id='ad639f7b-8116-4bcf-9dbf-9941d01bdbb3', content=[{'type': 'text', 'text': '16', 'annotations': None}], error=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llm_agents_from_scratch.data_structures import ToolCall\n",
    "\n",
    "provider = MCPToolProvider(\n",
    "    name=\"hailstone\",\n",
    "    stdio_params=server_params,\n",
    ")\n",
    "tools = await provider.get_tools()\n",
    "\n",
    "tool_call = ToolCall(\n",
    "    tool_name=tools[0].name,\n",
    "    arguments={\"x\": 5},\n",
    ")\n",
    "result = await tools[0](tool_call)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cdbedf-00bd-4ae7-a617-9d13661d9085",
   "metadata": {},
   "source": [
    "### Example 7: Manual discovery of MCP tools to construct an LLM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11b19865-a2c4-41c8-9476-01776a0e1340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_agents_from_scratch.llms import OllamaLLM\n",
    "from llm_agents_from_scratch import LLMAgent\n",
    "\n",
    "provider = MCPToolProvider(\n",
    "    name=\"hailstone\",\n",
    "    stdio_params=server_params,\n",
    ")\n",
    "tools = await provider.get_tools()\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "llm_agent = LLMAgent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2dc7246-5f8c-4740-b50d-3ed47124acd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llm_agents_from_scratch.tools.mcp.tool.MCPTool at 0x7b13d892eea0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_agent.tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c614d7-84b4-4406-80e3-90dae323178c",
   "metadata": {},
   "source": [
    "### Example 8: Different ways to construct an LLMAgentBuilder with the same attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f18fe5a7-dd2d-483f-8084-50a172cd0082",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_agents_from_scratch.llms import OllamaLLM\n",
    "from llm_agents_from_scratch.agent.builder import LLMAgentBuilder\n",
    "\n",
    "provider = MCPToolProvider(\n",
    "    name=\"hailstone\",\n",
    "    stdio_params=server_params,\n",
    ")\n",
    "llm = OllamaLLM(model=\"qwen2.5:3b\")\n",
    "\n",
    "# directly passing attributes at construction\n",
    "builder = LLMAgentBuilder(\n",
    "    llm=llm,\n",
    "    mcp_providers=[provider],\n",
    ")\n",
    "\n",
    "# fluent style with builder methods\n",
    "builder = LLMAgentBuilder()\\\n",
    "    .with_llm(llm)\\\n",
    "    .with_mcp_provider(provider)\n",
    "\n",
    "# another fluent style\n",
    "builder = LLMAgentBuilder()\\\n",
    "    .with_mcp_providers([provider])\\\n",
    "    .with_llm(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911813be-20c5-4d03-a17f-610c04f55f8e",
   "metadata": {},
   "source": [
    "### Example 9: Using an LLMAgentBuilder to create the same LLMAgent from example 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43dbee5f-d18d-4fc8-96b3-a2ba2eabef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm_agents_from_scratch.llms import OllamaLLM\n",
    "from llm_agents_from_scratch.agent.builder import LLMAgentBuilder\n",
    "\n",
    "provider = MCPToolProvider(\n",
    "    name=\"hailstone\",\n",
    "    stdio_params=server_params,\n",
    ")\n",
    "llm = OllamaLLM(model=\"qwen3:4b\")\n",
    "\n",
    "llm_agent = await LLMAgentBuilder()\\\n",
    "    .with_llm(llm)\\\n",
    "    .with_mcp_provider(provider)\\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2022752d-1e2a-4b63-9e9f-1b35ed2f0411",
   "metadata": {},
   "source": [
    "### Example 10: Performing a hailstone step with our LLM agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dddb93ee-e922-49b7-bdcd-cb0eb53044cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from llm_agents_from_scratch.logger import enable_console_logging\n",
    "\n",
    "enable_console_logging(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8344c6d1-9808-4f31-9a5e-d3ccc5be7c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO (llm_agents_fs.LLMAgent) :      üöÄ Starting task: What is hailstone_step_fn(5)? You must call the tool to get the answer.\n",
      "INFO (llm_agents_fs.TaskHandler) :      ‚öôÔ∏è Processing Step: What is hailstone_step_fn(5)? You must call the tool to get the answer.\n",
      "INFO (llm_agents_fs.TaskHandler) :      üõ†Ô∏è Executing Tool Call: mcp__hailstone__hailstone_step_fn\n",
      "INFO (llm_agents_fs.TaskHandler) :      ‚úÖ Successful Tool Call: [{'type': 'text', 'text': '16', 'annotations': None}]\n",
      "INFO (llm_agents_fs.TaskHandler) :      ‚úÖ Step Result: Okay, let's see. The user asked for hailstone_step_fn(5). I called the tool with x=5 and got the response 16.\n",
      "\n",
      "So the Hailstone sequenc...[TRUNCATED]\n",
      "INFO (llm_agents_fs.TaskHandler) :      üß† New Step: The assistant has already completed the tool call and received the result. The final answer is 16.\n",
      "INFO (llm_agents_fs.TaskHandler) :      ‚öôÔ∏è Processing Step: The assistant has already completed the tool call and received the result. The final answer is 16.\n",
      "INFO (llm_agents_fs.TaskHandler) :      ‚úÖ Step Result: Okay, let me check what's going on here. The user asked for hailstone_step_fn(5). The assistant already called the tool with x=5 and go...[TRUNCATED]\n",
      "INFO (llm_agents_fs.TaskHandler) :      No new step required.\n",
      "INFO (llm_agents_fs.LLMAgent) :      üèÅ Task completed: The answer to hailstone_step_fn(5) is **16**. This follows the Hailstone sequence rule where odd numbers $ x $ are transformed to $ ...[TRUNCATED]\n"
     ]
    }
   ],
   "source": [
    "from llm_agents_from_scratch.data_structures.agent import Task\n",
    "\n",
    "instruction = (\n",
    "    \"What is hailstone_step_fn(5)? \"\n",
    "    \"You must call the tool to get the answer.\"\n",
    ")\n",
    "task = Task(instruction=instruction)\n",
    "result = await llm_agent.run(task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c15a1d4-1ac9-4d6e-9512-7052bd71a999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskResult(task_id='cc267218-2e01-430f-b740-7bb1f9b50970', content='The answer to hailstone_step_fn(5) is **16**. This follows the Hailstone sequence rule where odd numbers $ x $ are transformed to $ 3x + 1 $. For $ x = 5 $: $ 3 \\\\times 5 + 1 = 16 $. No further tool calls are needed. Final answer: **16**.')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
